{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We Rate Dogs Wrangle Report\n",
    "**By Marie-Luise Klaus**  \n",
    "20/04/2019 Berlin, Germany   \n",
    "**for internal use**\n",
    "\n",
    "## 1.  Introduction\n",
    "This report describes the wrangling efforts of the We Rate Dogs data analysis project within the Udacity Data Analysis Nanodegree program. The data used refers to the WeRateDogs Twitter account, which humorously introduces dogs. \n",
    "\n",
    "https://twitter.com/dog_rates/  \n",
    "\n",
    "Below, we will introduce the steps throughout the wrangling process - gathering, assessing and cleaning data. Further, we will go briefly into detail about particularities to this data set. The wrangling itself can be found in the wrangling section of *'wrangle_act.ipyn'*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gathering Data\n",
    "The data was gathered from 3 different sources using different methodologies.\n",
    "\n",
    "### 2.1 WeRateDogs Twitter Archive\n",
    "Udacity provided a file *'twitter-archive-enhanced.csv'*, containing information about WeRateDogs tweets. We read in the csv file into a pandas dataframe: *'twitter_archive'*.  \n",
    "\n",
    "### 2.2 Image Predictions Data\n",
    "We programatically downloaded a tsv using Pythons request library from \n",
    "\n",
    "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv. \n",
    "\n",
    "The file was provided by Udacity and contains predictions about dog breeds occurring in the WeRateDog tweets, which was created using a neural network. We saved the file locally as *'image-predictions.tsv'* and loaded it into a pandas dataframe '*image_predictions'* thereafter.\n",
    "\n",
    "### 2.3 Twitter API Data\n",
    "The third source of data comes from Twitter's API. We created a Twitter developer's account and used the Python library Tweepy for the API requests. To locate relevant tweets, we used tweet_ids from the twitter archive. During the request, we missed 19 tweets due to deleted tweets. Subsequently, we saved the JSON-formatted tweet data to a file *'tweet_json.txt'*, line-by-line. We continued by reading in our file and parsing the data into a pandas dataframe *'tweets_df'*.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Assessing Data\n",
    "This section involved assessing data for quality and tidiness issues. We started by visually assessing our data sheets in Jupyter Notebook and Excel. To get a first understanding we used methodologies like .info() and .describe().  \n",
    "Thereafter, we got into detail by programmatically assessing dataframe by dataframe.\n",
    "\n",
    "**Issues**  \n",
    "The assessment section ends with a documentation of unclean data issues found during assessment. 12 quality issues and 2 tidiness issues aroused. The quality issues spread across all 3 dataframes. However, the Twitter archive appears to be the dirtiest. \n",
    "\n",
    "We found irrelevant tweets that didn't serve the purpose of the account, which is to rate dogs using a specific pattern. Mostly, corrupt replies were retweets and replies to other tweets, or tweets not related to any dog content.  We also found missing data as one of the main issues. Dogs were not named, wrongly or not classified into their 'stage' or incorrectly rated. There were incorrect datatypes and impractical formats. Regardless of us addressing 12 content issues throughout the cleaning process, there are more issues that could be cleaned, depending on the desired outcome of the analysis.  \n",
    "\n",
    "On the side of tidiness/structural issues, we found that dog stages were split in multiple stages across the Twitter archive instead of being in one column. To make a comprehensive analysis possible, we found it made sense to have all 3 datasets available in one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cleaning Data \n",
    "In this section we took a programmatic approach to clean our data from quality and tidiness issues found during assessment.\n",
    "We began by copying each dataframe to work with in oder to preserve the original data. Thereafter, we began cleaning each issue taking three steps: \n",
    "1. **Define** how to clean up the issue, \n",
    "2. Write **Code** to fix the issue and then \n",
    "3. **Test** whether the intended result was successful without any unwanted side effects. \n",
    "In the course of this, we took a test-driven development approach (TDD), meaning we took step 3 before step 2.  \n",
    "\n",
    "We started by fixing the tidiness issues first. As a result we generated a combined dataframe *master_df*, which was then used to fix quality issues. \n",
    "\n",
    "The assessment and cleaning of the dataframe were not straightforward. While cleaning, we found further issues, which made us go back and forth between assessment to cleaning processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    " As a result of the wrangling, we removed corrupt, inaccurate and unnecessary record in preparation for the following analysis and visualization. The clean master dataframe was stored in a the file *'twitter_archive_master.csv'*. By the end of the cleaning process, we managed to keep 1670 out of the 2356 records, which was sufficient for the purpose of our analysis and visualization. \n",
    " \n",
    "Despite our efforts and the iterative process of wrangling, we see potential for further cleaning of the data. One critical point that we did not manage to clean, was it's up-to-dateness. The youngest tweet is almost 2 years old and it would have been interesting to find out the WeRateDogs account performed over a longer time frame. \n",
    " \n",
    "In general, the cleaner a dataframe, the more accurate are the final conclusions. Data wrangling processes aim to prepare it's data, depending on it's desired analysis. And for our analysis the wrangling resulted in sufficient data to find interesting conclusions.    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
